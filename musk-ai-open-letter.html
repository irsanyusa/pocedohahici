<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="An open letter with signatures from hundreds of the biggest names in tech, including Elon Musk, has urged the worlds leading artificial intelligence labs to pause the training of new super-powerful systems for six months, saying that recent advances in AI present profound risks to society and humanity."><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Elon Musk Signs Open Letter Urging AI Labs to Pump the Brakes</title><link rel=canonical href=./musk-ai-open-letter.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>JadeSync</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Elon Musk Signs Open Letter Urging AI Labs to Pump the Brakes</h1><div id=sub-header>August 2024 · 4 minute read</div><div class=entry-content><img src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2023/03/GettyImages-1246506636.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><span class="leading-7 float-left border-t-2 border-l-2 border-solid border-time-red text-5xl py-2 pr-0.5 pl-[0.3125rem] my-0.5 mr-2.5 font-zilla-slab">A</span>n open letter with signatures from hundreds of the biggest names in tech, including Elon Musk, has urged the world’s leading artificial intelligence labs to pause the training of new super-powerful systems for six months, saying that recent advances in AI present “profound risks to society and humanity.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The letter comes just two weeks after the public release of OpenAI’s <a href=#>GPT-4,</a> the most powerful AI system ever released, which has led researchers to slash their expectations for when AGI—or artificial general intelligence that surpasses human cognitive ability—will arrive. Many experts fear that, as an AI arms race heats up, humanity is sleepwalking into catastrophe.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More: </strong><a href=#>The AI Arms Race Is Changing Everything</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources,” the letter says. “Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The list of signatories includes the names of more than 1,000 people alongside Musk, including Apple co-founder Steve Wozniak, <i>Sapiens</i> author Yuval Noah Harari, and some of AI’s most distinguished academics responsible for multiple breakthroughs in machine learning. As of Tuesday, no OpenAI employees had signed the letter, although CEO Sam Altman’s name briefly appeared then disappeared from the list of signatories. At least four Google employees, including three from its subsidiary AI lab DeepMind, Emad Mostaque, the CEO of Stability AI and Tristan Harris, executive director of the Center for Humane Technology also appeared on the list. TIME was not able to verify that all signatures were genuine.</p><h3 class="text-[1.17rem] font-bold tracking-0.5px font-zilla-slab self-baseline">More from TIME</h3><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More:</strong> <a href=#>Elon Musk Is Bringing the Culture Wars to AI</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“Contemporary AI systems are now becoming human-competitive at general tasks,” states the letter, which was hosted on the Future of Life Institute’s website. “We must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More: </strong><a href=#>The New AI-Powered Bing Is Threatening Users. That’s No Laughing Matter</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The letter calls on all AI labs to agree to use the proposed six-month pause to continue with safety research. “AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts,” it says. “These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt.” Those behind the letter note that they are not calling for a AI development in general to be paused, rather “a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More: </strong><a href=#>Why Bill Gates Believes Generative AI Will Be ‘Revolutionary’</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Gary Marcus, a signatory of the open letter and author of the book <i>Rebooting AI</i>, told TIME he added his name because he felt a responsibility to speak out. “There are serious near-term and far-term risks and corporate AI responsibility seems to have lost fashion right when humanity needs it most,” he said. “Blundering so quickly into uncharted territory with so much at stake doesn’t seem like a great way to go. If the [corporations] won’t take these risks seriously enough, it is important for the rest of us to speak up.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More:</strong> <a href=#>AI-Human Romances Are Flourishing—And This Is Just the Beginning</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Simeon Campos, the CEO of AI safety startup SaferAI, told TIME he signed the letter because it is impossible to manage the risks of systems when even the inventors of those systems don’t know exactly how they work, don’t know what they’re capable of, and don’t know how to place limits on their behavior. “What are we currently doing?” Campos said. “We’re scaling such systems to unprecedented levels of capabilities in a race at full speed with transformative effects on society. We must slow down the development of these systems to let society adapt and accelerate alternative AGI architectures that are safe and formally verifiable by design.”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The open letter ends on a hopeful note. “Society has hit pause [before] on other technologies with potentially catastrophic effects on society. We can do so here. Let’s enjoy a long AI summer, not rush unprepared into a fall.”</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmismaKyb6%2FOpmZvamZrg3iFjqasrKNdlrZuu8%2BepWaklanBpr6O</p></div><div id=links><a href=./la-halde-rappelle-la-condition-de-domicile-reel-pour-la-delivrance-de-la-carte-didentite.html>&#171;&nbsp;La Halde rappelle la condition de domicile rel pour la dlivrance de la carte d'identit</a>
<a href=./east-new-york-canceled-cbs-1235604861.html>East New York, True Lies Both Canceled at CBS After One Season&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>